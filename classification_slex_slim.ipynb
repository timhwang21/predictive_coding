{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import Dataset\n",
    "from model import Model\n",
    "\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import shutil\n",
    "from itertools import product\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start:', datetime.now(pytz.timezone('US/Eastern')).strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIT_COMMIT_HASH = os.popen('git rev-parse --short HEAD').read().replace('\\n', '')\n",
    "print(GIT_COMMIT_HASH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Paramters and Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = pd.read_pickle('parameters.pkl')\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS = {'USE_MASK': [False],\n",
    "#           'GAUSS_MASK_SIGMA': [1.0],\n",
    "#           'IMAGE_FILTER': [(-1,1)],\n",
    "#           'DOG_KSIZE': [(5,5)],\n",
    "#           'DOG_SIGMA1': [1.3],\n",
    "#           'DOG_SIGMA2': [2.6],\n",
    "#           'INPUT_SCALE': [1.0],\n",
    "#           'ITER_N': [1],\n",
    "#           'EPOCH_N': [100],\n",
    "#           'CLEAR_SAVED_WEIGHTS': [True],\n",
    "#           'IN_DIR': ['slex_len3_small'],\n",
    "#           'OUT_DIR': ['slex_len3_small_results'],\n",
    "#           'RF1_SIZE': [{'x': 1, 'y': 3}],\n",
    "#           'RF1_OFFSET': [{'x': 1, 'y': 3}],\n",
    "#           'RF1_LAYOUT': [{'x': 1, 'y': 7}],\n",
    "#           'LEVEL1_MODULE_SIZE': [32],\n",
    "#           'LEVEL2_MODULE_SIZE': [128],\n",
    "#           'ALPHA_R': [0.1],\n",
    "#           'ALPHA_U': [0.1],\n",
    "#           'ALPHA_V': [0.1],\n",
    "#           'ALPHA_DECAY': [1],\n",
    "#           'ALPHA_MIN': [0],\n",
    "#           'TEST_INTERVAL': [10]}\n",
    "\n",
    "# param = pd.DataFrame(PARAMS).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "in_dir = os.path.join(os.path.abspath('.'), 'data', param.IN_DIR)\n",
    "out_dir = os.path.join(os.path.abspath('.'), param.OUT_DIR)\n",
    "\n",
    "test_set = Dataset(scale=param.INPUT_SCALE,\n",
    "                   shuffle=False,\n",
    "                   data_dir=in_dir,\n",
    "                   rf1_x=param.RF1_SIZE['x'],\n",
    "                   rf1_y=param.RF1_SIZE['y'],\n",
    "                   rf1_offset_x=param.RF1_OFFSET['x'],\n",
    "                   rf1_offset_y=param.RF1_OFFSET['y'],\n",
    "                   rf1_layout_x=param.RF1_LAYOUT['x'],\n",
    "                   rf1_layout_y=param.RF1_LAYOUT['y'],\n",
    "                   use_mask=param.USE_MASK,\n",
    "                   gauss_mask_sigma=param.GAUSS_MASK_SIGMA,\n",
    "                   image_filter=param.IMAGE_FILTER,\n",
    "                   DoG_ksize=param.DOG_KSIZE,\n",
    "                   DoG_sigma1=param.DOG_SIGMA1,\n",
    "                   DoG_sigma2=param.DOG_SIGMA2)\n",
    "\n",
    "test_int = param.TEST_INTERVAL\n",
    "\n",
    "epoch_n = param.EPOCH_N\n",
    "zero_pad_len = len(str(epoch_n))\n",
    "\n",
    "model = Model(dataset=test_set,\n",
    "              level1_module_size=param.LEVEL1_MODULE_SIZE,\n",
    "              level2_module_size=param.LEVEL2_MODULE_SIZE)\n",
    "\n",
    "# parameters\n",
    "model.iteration = param.ITER_N\n",
    "\n",
    "model.alpha_r = param.ALPHA_R\n",
    "model.alpha_u = param.ALPHA_U\n",
    "model.alpha_v = param.ALPHA_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "if param.CLEAR_SAVED_WEIGHTS == True and os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "# determining which set of weights to use\n",
    "\n",
    "out_dir_all = glob.glob(os.path.join(out_dir, '*'))\n",
    "out_dir_epoch = glob.glob(os.path.join(out_dir, 'epoch_*'))\n",
    "out_dir_pretrain = os.path.join(out_dir, 'pretraining')\n",
    "\n",
    "if len(out_dir_epoch) > 0:\n",
    "    # load weights from previous results\n",
    "    regex = re.compile(os.path.join(out_dir, 'epoch_(?P<epoch>.*)'))\n",
    "    epoch_all = [int(regex.match(x).group('epoch')) for x in out_dir_epoch]\n",
    "    epoch_max_idx = np.argmax(epoch_all)\n",
    "    epoch_max = epoch_all[epoch_max_idx]\n",
    "    model.load(out_dir_epoch[epoch_max_idx])\n",
    "elif out_dir_pretrain not in out_dir_all:\n",
    "    # save current pretraining weights\n",
    "    epoch_max = -1\n",
    "    model.save(out_dir_pretrain)\n",
    "else:\n",
    "    # load previous pretraining weights\n",
    "    epoch_max = -1\n",
    "    model.save(out_dir_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay over epochs\n",
    "u = np.array([param.ALPHA_U/(param.ALPHA_DECAY**i) for i in range(param.EPOCH_N)])\n",
    "v = np.array([param.ALPHA_V/(param.ALPHA_DECAY**i) for i in range(param.EPOCH_N)])\n",
    "u[u < param.ALPHA_MIN] = param.ALPHA_MIN\n",
    "v[v < param.ALPHA_MIN] = param.ALPHA_MIN\n",
    "plt.plot(u, label='u');\n",
    "plt.plot(v, label='v');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for epoch in (x for x in range(epoch_n) if x > epoch_max):\n",
    "    # learning rate\n",
    "    model.alpha_u = u[epoch]\n",
    "    model.alpha_v = v[epoch]\n",
    "    \n",
    "    # images are shuffled for each training epoch\n",
    "    train_set = Dataset(scale=param.INPUT_SCALE,\n",
    "                        shuffle=True,\n",
    "                        data_dir=in_dir,\n",
    "                        rf1_x=param.RF1_SIZE['x'],\n",
    "                        rf1_y=param.RF1_SIZE['y'],\n",
    "                        rf1_offset_x=param.RF1_OFFSET['x'],\n",
    "                        rf1_offset_y=param.RF1_OFFSET['y'],\n",
    "                        rf1_layout_x=param.RF1_LAYOUT['x'],\n",
    "                        rf1_layout_y=param.RF1_LAYOUT['y'],\n",
    "                        use_mask=param.USE_MASK,\n",
    "                        gauss_mask_sigma=param.GAUSS_MASK_SIGMA,\n",
    "                        image_filter=param.IMAGE_FILTER,\n",
    "                        DoG_ksize=param.DOG_KSIZE,\n",
    "                        DoG_sigma1=param.DOG_SIGMA1,\n",
    "                        DoG_sigma2=param.DOG_SIGMA2)\n",
    "\n",
    "    # replaced model.train(train_set)\n",
    "    for word_i in range(train_set.rf2_patches.shape[0]):\n",
    "        inputs = train_set.rf2_patches[word_i]\n",
    "        labels = train_set.labels[word_i]\n",
    "\n",
    "        # HACK: remove rf2 patches where all values are identical (assuming no variation = silence or no information)\n",
    "        bool_mask = np.max(inputs, axis=(2,3)) != np.min(inputs, axis=(2,3))\n",
    "        mask_y = bool_mask.any(axis=1).sum()\n",
    "        mask_x = bool_mask.any(axis=0).sum()\n",
    "        \n",
    "        inputs = inputs[bool_mask].reshape((mask_y, mask_x) + inputs.shape[2:])\n",
    "        labels = labels[bool_mask].reshape((mask_y, mask_x) + labels.shape[2:])\n",
    "\n",
    "        output = pd.DataFrame.from_dict(model.apply_input(inputs, labels, train_set, training=True))\n",
    "\n",
    "    if epoch == 0 or epoch % test_int == test_int-1:\n",
    "        model.save(os.path.join(out_dir, 'epoch_{:0>{}d}'.format(epoch, zero_pad_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(glob.glob(os.path.join(in_dir, '*.png')))\n",
    "\n",
    "regex = re.compile(os.path.join(in_dir, '(?P<index>.*)_(?P<word>.*).png'))\n",
    "f_index = [regex.match(x).group('index') for x in filenames]\n",
    "f_word = [regex.match(x).group('word') for x in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose 10 words to plot\n",
    "word_select = sorted(np.random.choice(len(test_set.filtered_images), 10, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Simulation Results as a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data by epoch for parallelization\n",
    "\n",
    "def load_data(epoch, iteration):\n",
    "    \n",
    "    results = {'epoch': [], 'target': [], 'idx': [], 'last_idx': [], 'iteration': [],\n",
    "               'node': [], 'node_word': [], 'target_label': [],\n",
    "               'activation_raw': [], 'activation': [],\n",
    "               'target_n': [], 'response_n': [], 'accuracy': []}\n",
    "\n",
    "    filenames = sorted(glob.glob(os.path.join(out_dir, 'epoch_*')))\n",
    "    regex = re.compile(os.path.join(out_dir, 'epoch_(?P<pad>0*)(?P<epoch>.+)'))\n",
    "    f_epoch = [int(regex.match(x).group('epoch')) for x in filenames]\n",
    "    \n",
    "    model.load(filenames[f_epoch.index(epoch)])\n",
    "    \n",
    "    for word_i in np.ndindex(test_set.rf2_patches.shape[0]):\n",
    "\n",
    "        inputs = test_set.rf2_patches[word_i]\n",
    "        labels = test_set.labels[word_i]\n",
    "\n",
    "        # HACK: remove rf2 patches where all values are identical (assuming no variation = silence or no information)\n",
    "        bool_mask = np.max(inputs, axis=(2,3)) != np.min(inputs, axis=(2,3))\n",
    "        mask_y = bool_mask.any(axis=1).sum()\n",
    "        mask_x = bool_mask.any(axis=0).sum()\n",
    "        inputs = inputs[bool_mask].reshape((mask_y, mask_x) + inputs.shape[2:])\n",
    "        labels = labels[bool_mask].reshape((mask_y, mask_x) + labels.shape[2:])\n",
    "\n",
    "        output = pd.DataFrame.from_dict(model.apply_input(inputs, labels, test_set, training=False))\n",
    "        \n",
    "        idx_list = [x for x in output.index if output.iteration[x] == iteration]\n",
    "        \n",
    "        for idx in idx_list:\n",
    "            last_idx = True if idx == max(idx_list) else False\n",
    "        \n",
    "            target_n = np.argmax(output.label[idx])\n",
    "\n",
    "            r3_raw = output.r3[idx].astype(np.float128)\n",
    "            r3 = np.exp(r3_raw)/np.sum(np.exp(r3_raw))\n",
    "\n",
    "            if sum(r3 == r3.max()) != 1:\n",
    "                response_n = None\n",
    "            else:\n",
    "                response_n = np.argmax(r3)\n",
    "\n",
    "            if target_n == response_n:\n",
    "                accuracy = 1\n",
    "            else:\n",
    "                accuracy = 0\n",
    "\n",
    "            results['epoch'] += [epoch] * len(r3)\n",
    "            results['target'] += [f_word[target_n]] * len(r3)\n",
    "            results['idx'] += [idx] * len(r3)\n",
    "            results['last_idx'] += [last_idx] * len(r3)\n",
    "            results['iteration'] += [output.iteration[idx]] * len(r3)\n",
    "            results['node'] += list(range(len(r3)))\n",
    "            results['node_word'] += [f_word[x] for x in list(range(len(r3)))]\n",
    "            results['target_label'] += list(output.label[idx])\n",
    "            results['activation_raw'] += list(r3_raw)\n",
    "            results['activation'] += list(r3)\n",
    "            results['target_n'] += [target_n] * len(r3)\n",
    "            results['response_n'] += [response_n] * len(r3)\n",
    "            results['accuracy'] += [accuracy] * len(r3)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "pool = mp.Pool(8) # may increase number of CPU if available\n",
    "\n",
    "epoch_list = (x for x in range(epoch_n) if x == 0 or x % test_int == test_int-1)\n",
    "iter_list = (x for x in range(model.iteration) if x == max(range(model.iteration)))\n",
    "\n",
    "df_list = pool.starmap(load_data, product(epoch_list, iter_list))\n",
    "\n",
    "results_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_given_target(target, word):\n",
    "\n",
    "    if target == word:\n",
    "        category = 'target'\n",
    "    elif target[0:2] == word[0:2]:\n",
    "        category = 'cohort'\n",
    "    elif target[1:] == word[1:]:\n",
    "        category = 'rhyme'\n",
    "    elif word in target:\n",
    "        category = 'embedded'\n",
    "    else:\n",
    "        category = 'other'\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_combo_df = pd.DataFrame(list(product(f_word, f_word)), columns=['target','node_word'])\n",
    "\n",
    "word_combo_df['category'] = word_combo_df.apply(lambda x: category_given_target(x['target'], x['node_word']), axis = 1)\n",
    "cat_list = ['target', 'cohort', 'rhyme', 'other', 'embedded']\n",
    "word_combo_df.category = word_combo_df.category.astype('category').cat.set_categories(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.merge(results_df, word_combo_df, on=['target', 'node_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(os.path.join(out_dir, 'results.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_pickle(os.path.join(out_dir, 'results.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Change by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "ALL_WEIGHTS = {\"epoch\": [], \"U1\": [], \"U2\": [], \"U3\": [], \"V1\": [], \"V2\": [], \"V3\": []}\n",
    "\n",
    "model.load(out_dir_pretrain)\n",
    "\n",
    "ALL_WEIGHTS[\"epoch\"].append(-1)\n",
    "ALL_WEIGHTS[\"U1\"].append(model.U1.mean())\n",
    "ALL_WEIGHTS[\"U2\"].append(model.U2.mean())\n",
    "ALL_WEIGHTS[\"U3\"].append(model.U3.mean())\n",
    "ALL_WEIGHTS[\"V1\"].append(model.V1.mean())\n",
    "ALL_WEIGHTS[\"V2\"].append(model.V2.mean())\n",
    "ALL_WEIGHTS[\"V3\"].append(model.V3.mean())\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    if epoch % test_int == test_int-1:\n",
    "        filenames = sorted(glob.glob(os.path.join(out_dir, 'epoch_*')))\n",
    "        regex = re.compile(os.path.join(out_dir, 'epoch_(?P<pad>0*)(?P<epoch>.+)'))\n",
    "        f_epoch = [int(regex.match(x).group('epoch')) for x in filenames]\n",
    "\n",
    "        model.load(filenames[f_epoch.index(epoch)])\n",
    "        \n",
    "        ALL_WEIGHTS[\"epoch\"].append(epoch)\n",
    "        ALL_WEIGHTS[\"U1\"].append(model.U1.mean())\n",
    "        ALL_WEIGHTS[\"U2\"].append(model.U2.mean())\n",
    "        ALL_WEIGHTS[\"U3\"].append(model.U3.mean())\n",
    "        ALL_WEIGHTS[\"V1\"].append(model.V1.mean())\n",
    "        ALL_WEIGHTS[\"V2\"].append(model.V2.mean())\n",
    "        ALL_WEIGHTS[\"V3\"].append(model.V3.mean())\n",
    "        \n",
    "ALL_WEIGHTS_DF = pd.DataFrame.from_dict(ALL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WEIGHTS_DF.groupby('epoch').mean().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df.last_idx == True)].groupby('epoch')['accuracy'].mean().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy by Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df[(results_df.last_idx == True)].groupby('target')['accuracy'].mean().plot.bar(figsize=(40,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation of Top 10 Activated Items for a Given Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_cutoff = int(epoch_n * 1/5)\n",
    "epoch_cutoff = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch >= epoch_cutoff) & (results_df.last_idx == True)]\n",
    "    top_10 = results_df_select[(results_df_select.last_idx == True) & (results_df_select.epoch == max(results_df_select.epoch))].sort_values(by=['activation'], ascending=False).node_word[0:10]\n",
    "    results_df_select = results_df_select.loc[results_df_select.node_word.isin(top_10)]\n",
    "    \n",
    "    # order based on average activation values\n",
    "    results_df_select.node_word = results_df_select.node_word.astype('category').cat.set_categories(top_10)\n",
    "    \n",
    "    # plot\n",
    "    df_plot = results_df_select.groupby(['epoch','node_word']).mean()['activation'].unstack().plot(title='target: {}'.format(f_word[word_i]));\n",
    "    \n",
    "    # thicken target line\n",
    "    lws = results_df_select.groupby('node_word').mean()['target_label']*2+1\n",
    "    \n",
    "    for i, l in enumerate(df_plot.lines):\n",
    "        plt.setp(l, linewidth=lws[i])\n",
    "        \n",
    "    df_plot.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Timesteps at the Last Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch == max(results_df.epoch))]\n",
    "    top_10 = results_df_select[(results_df_select.last_idx == True) & (results_df_select.epoch == max(results_df_select.epoch))].sort_values(by=['activation'], ascending=False).node_word[0:10]\n",
    "    results_df_select = results_df_select[results_df_select.node_word.isin(top_10)]\n",
    "    \n",
    "    # order based on average activation values\n",
    "    results_df_select.node_word = results_df_select.node_word.astype('category').cat.set_categories(top_10)\n",
    "\n",
    "    # plot\n",
    "    df_plot = results_df_select.groupby(['idx','node_word']).mean()['activation'].unstack().plot(title='target: {}'.format(f_word[word_i]));\n",
    "    \n",
    "    # thicken target line\n",
    "    lws = results_df_select.groupby('node_word').mean()['target_label']*2+1\n",
    "    \n",
    "    for i, l in enumerate(df_plot.lines):\n",
    "        plt.setp(l, linewidth=lws[i])\n",
    "        \n",
    "    df_plot.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Across All Items Over Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"epoch\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=None,\n",
    "             data=results_df[(results_df.last_idx == True) & (results_df.epoch >= epoch_cutoff)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Across All Items Over Timesteps at the Last Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"idx\", y=\"activation\", hue=\"category\", hue_order=cat_list, err_style=\"band\",\n",
    "             data=results_df[(results_df.epoch == max(range(epoch_n)))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Item Over Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch >= epoch_cutoff) & (results_df.last_idx == True)]\n",
    "    df_plot = results_df_select.groupby(['epoch','category']).mean()['activation'].unstack().plot(title='target: {}'.format(f_word[word_i]));\n",
    "    df_plot.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Timesteps at the Last Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_i in word_select:\n",
    "    results_df_select = results_df[(results_df.target_n == word_i) & (results_df.epoch == max(results_df.epoch))]\n",
    "    df_plot = results_df_select.groupby(['idx','category']).mean()['activation'].unstack().plot(title='target: {}'.format(f_word[word_i]));\n",
    "    df_plot.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('End:', datetime.now(pytz.timezone('US/Eastern')).strftime('%c'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
